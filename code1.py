# -*- coding: utf-8 -*-
"""code1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ixwAgDZSPwEHotpABrf9g13ttX5TeZDV
"""

import numpy as np
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
import keras
import pandas as pd
import matplotlib.pyplot as plt

under_boys = np.load('/content/drive/MyDrive/Datathon/under_boys.npy')
under_girls = np.load('/content/drive/MyDrive/Datathon/under_girls.npy')

boys = np.zeros(20)
girls = np.zeros(20)
print(under_boys.shape, boys.shape)

for i in range(20):
  for j in range(254):
    boys[i] += under_boys[j,i]
    girls[i] += under_girls[j,i]
both_genders = boys + girls

print(years)

years = np.zeros(20)
for i in range(20):
  years[i]=1969+i

# Project NCHS underweight babys to 2030
# a bunch of (year, # underweight babys)
both_genders = boys + girls
model = np.poly1d(np.polyfit(years, both_genders, 2))


plt.scatter(years, both_genders)
myline = np.linspace(1969, 2030, len(both_genders))
plt.plot(myline, model(myline))
#plt.ylim([20000, 80000])
plt.show()

scipy.optimize.curve_fit()

"""Time series from Sean's sql queries"""

under_boys = np.array([[20,30,40], [1,2,3]])
under_girls = np.array([[25,35,45], [1,2,3]])
both_genders = under_boys + under_girls
model = np.poly1d(np.polyfit(under_boys[1], both_genders[0], 2))


plt.scatter(under_boys[1], both_genders[0])
myline = np.linspace(1, 22, len(both_genders[0]))
plt.plot(myline, model(myline))
plt.show()

# a bunch of (year, # underweight babys)
x = [1967, ..., 1987]
y = []
for year in years:
  num = 0
  for row in year:
    if weight < 5lbs:
      num += 1
  y.append(num)

"""file paths- /content/drive/MyDrive/Datathon/US1969-1986/Natality.000

1.) Import Data
"""

# get state
df = df[df['State'].str.contains('Florida')]
# go through by county
countys = []
for index, row in df.iterrows():
  countys.append(row['County'])
countys = list(set(countys))

# county by county
dataframes = []
for county in countys:
  countydf = df[df['County'].str.contains(county)]
  dataframes.append(countydf)

features = ['ID_M_RACE', 'ID_F_RACE','DT_YEAR', 'ID_SEX', 'AM_M_AGE', 'AM_F_AGE', 'AM_TOT_B_ORDER', 'ID_M_EDU', 'ID_F_EDU', 'AM_GESTATION', 'STATE', 'COUNTY']

features = ['ID_M_RACE', 'ID_F_RACE','DT_YEAR', 'ID_SEX', 'AM_M_AGE', 'AM_F_AGE', 'AM_TOT_B_ORDER', 'ID_M_EDU', 'ID_F_EDU']
# Separating out the features
x = df.loc[:, features].values
# Separating out the target
y = df.loc[:,['AM_BIRTHWEIGHT']].values
# Standardizing the features
x = StandardScaler().fit_transform(x)

# label values to 8 bins
for i in range(len(y)):
  if y[i] > 4446:
    y[i] = 8
  if y[i] > 4340 and y[i]<=4446:
    y[i] = 7
  if y[i] > 4172 and y[i]<=4340:
    y[i] = 6
  if y[i] > 3879 and y[i]<=4172:
    y[i] = 5
  if y[i] > 3530 and y[i]<=3879:
    y[i] = 4
  if y[i] > 3150 and y[i]<=3530:
    y[i] = 3
  if y[i] > 2774 and y[i]<=3150:
    y[i] = 2
  if y[i]<=2774:
    y[i] = 1

print(frame["State"])

li = []
for filename in filenames:
    frame = pd.read_csv(filename, index_col=None, header=0)
    
    li.append(frame)

df = pd.concat(li, axis=0, ignore_index=True)

df = pd.read_csv ("/content/drive/MyDrive/Datathon/US1969-1986/Natality.000")

filenames = []
file_numbers = ["%03d" % x for x in range(100)]
for i in range(100):
  filenames.append("/content/drive/MyDrive/Datathon/US1969-1986/Natality." + file_numbers[i])

dataset = tf.data.Dataset.list_files(filenames, seed=42, shuffle=True)

# this reads 5 text files at a time, skips the first row of each file
dataset.interleave(lambda filename: tf.data.TextLineDataset(filename).skip(1), cycle_length=5, num_parallel_calls=tf.data.experimental.AUTOTUNE)

"""2.) Principal Componenet Analysis

3.) Create Model
"""

#define model
model = tf.keras.models.Sequential([
                               tf.keras.layers.Flatten(input_shape=(9,)),
                                   tf.keras.layers.Dense(128,activation='relu'),
                                   tf.keras.layers.Dropout(0.2),
                                   tf.keras.layers.Dense(8)
])

#define loss function variable
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

#define optimizer,loss function and evaluation metric
model.compile(optimizer='adam',
             loss=loss_fn,
             metrics=['accuracy'])
 
#train the model
model.fit(x,y,epochs=5)

"""4.) Visualize Results """



# quadratic regression
for i in range(len(x))
  model = np.poly1d(np.polyfit(x, y, 2))