# -*- coding: utf-8 -*-
"""code2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ig5kXU44yS2P41CTXXe5W31HowYp_DvO
"""

import numpy as np
import psycopg2
import csv
import plotly.figure_factory as ff
import pandas as pd
import tensorflow as tf
import keras
import sklearn
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

SES = np.load('/content/drive/MyDrive/Datathon/ses.npy')

################################################
        # Feautre Importance #
################################################

# idea 

# use scikit-learn logistic regression to 
# identify most important feautres

# X = socioeconomic_vector (education for mom and dad(0,17), race of child(0,8))
# y = binary classification for underweight (0: no 1: yes)
SES = np.asarray(SES)
X = np.zeros((len(SES),3))
y = np.zeros(len(SES))
for i in range(len(SES)):
    X[i,0] = SES[i,0]
    X[i,1] = SES[i,1]
    X[i,2] = SES[i,2]
    
    if SES[i,3] < 2548:
        y[i] = 1
    else:
        y[i] = 0

# define the model
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
model = LogisticRegression()

# fit the model
model.fit(X, y)

# get importance
importance = model.coef_[0]

# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
	
# plot feature importance
plt.bar([x for x in range(len(importance))], importance)
plt.show()

################################################
        # Deep Neural Network in Keras
################################################

X = np.zeros((len(SES),6))
y = np.zeros(len(SES))
for i in range(len(SES)):
    X[i,0] = SES[i,0]
    X[i,1] = SES[i,1]
    X[i,2] = SES[i,2]
    X[i,4] = SES[i,4]
    X[i,5] = SES[i,5]
    
    # label values to 8 bins
    if SES[i,3] > 4446:
        y[i] = 7
    if SES[i,3] > 4340 and SES[i,3]<=4446:
        y[i] = 6
    if SES[i,3] > 4172 and SES[i,3]<=4340:
        y[i] = 5
    if SES[i,3] > 3879 and SES[i,3]<=4172:
        y[i] = 4
    if SES[i,3] > 3530 and SES[i,3]<=3879:
        y[i] = 3
    if SES[i,3] > 3150 and SES[i,3]<=3530:
        y[i] = 3
    if SES[i,3] > 2774 and SES[i,3]<=3150:
        y[i] = 1
    if SES[i,3]<=2774:
        y[i] = 0

# split into training and test sets 
X = StandardScaler().fit_transform(X)
trainingX = X[0:3500817]
trainingy = y[0:3500817]
testX = X[3500817:4376022]
testy = y[3500817:4376022]

#define model
model = tf.keras.models.Sequential([
                               tf.keras.layers.Flatten(input_shape=(6,)),
                                   tf.keras.layers.Dense(128,activation='relu'),
                                   tf.keras.layers.Dense(128,activation='relu'),
                                   tf.keras.layers.Dense(8)
])

#define loss function variable
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

#define optimizer,loss function and evaluation metric
model.compile(optimizer='adam',
             loss=loss_fn,
             metrics=['accuracy'])

#train the model
model.fit(trainingX,trainingy,epochs=5)

################################################

model.evaluate(x=testX, y=testy)